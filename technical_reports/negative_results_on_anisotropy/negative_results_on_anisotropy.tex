% ==============================================================================
% FILE: paper/02_heuristic_failures/main.tex
% This is the final, publication-ready version of the Heuristics Falsification paper.
% It will also serve as Chapter 2 of the final dissertation.
% ==============================================================================
\documentclass[11pt, letterpaper, twoside]{article}

% --- PREAMBLE ---
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm, bm} % Math packages, bm for bold math
\usepackage{graphicx}
\usepackage[margin=1in, headheight=13.6pt]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{times}
\usepackage{xcolor}
\usepackage{booktabs} % For professional tables
\usepackage{caption}  % For better caption control
\usepackage{fancyhdr} % For headers
\usepackage{subcaption} % For subfigures

% --- STYLING ---
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue!50!black,
    urlcolor=blue!80!black,
}
\linespread{1.15}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{Axiom}

\pagestyle{fancy}
\fancyhf{} % clear all header and footers
\fancyhead[LE,RO]{\thepage}
\fancyhead[RE]{\textit{Why Hand-Crafted Anisotropic Diffusion Fails}}
\fancyhead[LO]{\textit{M. R. Desigan}}
\renewcommand{\headrulewidth}{0.4pt}

% --- TITLE ---
\title{\textbf{Negative Results that Illuminate Design: \\ A Failure Analysis of Hand-Crafted Anisotropic Graph Diffusion}}
\author{Mohana Rangan Desigan \\ \textit{Independent Research}}
\date{August 16, 2025}

% --- DOCUMENT START ---
\begin{document}
\maketitle

\begin{abstract}
The graph Laplacian's isotropic assumption—that information diffuses uniformly across a data manifold—is a foundational limitation of many graph neural networks and spectral methods. While intuitively incorrect, attempts to remedy this with hand-crafted anisotropic operators have remained ad-hoc and are rarely scrutinized. This work provides the first systematic failure analysis of a broad class of intuitive, geometry-aware anisotropic graph diffusion operators. We evaluate four such heuristics—hard curvature gating, connectivity-preserving sigmoid mapping, community-guided weighting, and curvature-rank modulation—on the 20 Newsgroups text clustering benchmark. Despite plausible geometric motivation, each approach fails in a distinct and illuminating manner: catastrophic fragmentation from hard gating, global geometric warping from a naive sigmoid mapping, critical information loss from coarse community partitioning, and insufficient spectral impact from rank-based nudging.

From these failures, we derive a set of formal axioms that any robust anisotropic operator must satisfy. We provide a suite of diagnostic visualizations and statistical evidence to formally show how each heuristic violates at least one of these axioms in practice. Our results establish a precise failure taxonomy, proving that the relationship between local manifold geometry and optimal global diffusion is too complex to be captured by simple, fixed heuristics. This rigorous negative result serves a crucial purpose: it definitively closes the door on a broad class of naive approaches and provides the formal motivation for a learnable paradigm, which we outline as the necessary path forward.
\end{abstract}

\section{Introduction: The Curse of Isotropy}
Graph-based representation learning methods model a dataset as a point cloud $\{x_i\}_{i=1}^N \subset \mathbb{R}^D$ residing on a low-dimensional manifold $(\mathcal{M}, g)$. The symmetric normalized Laplacian, $L = I - D^{-1/2}WD^{-1/2}$, serves as a discrete approximation of the Laplace-Beltrami operator $\Delta_g$. Its associated diffusion operator, the graph heat kernel $e^{-tL}$, forms the basis of the Spectral Graph Wavelet Transform (SGWT) \cite{hammond2011wavelets}. While powerful, this operator is fundamentally \textbf{isotropic}, assuming a uniform diffusion process. This is misaligned with real-world data, where semantic "threads" should have high conductance and inter-topic "bridges" should have low conductance. This paper investigates the intuitive but perilous path of manually designing an anisotropic operator to fix this flaw.

\section{Axioms for Anisotropic Operators}
Based on our subsequent failure analysis, we propose that any well-behaved anisotropic operator, which modifies an isotropic weight matrix $W$ to an anisotropic $W'$, must satisfy the following axioms. Let $w'_{ij} = w_{ij} \cdot h(\cdot)$, where $h$ is a modulation function.

\begin{axiom}[Connectivity Preservation]
For a connected graph $G=(V,E,W)$, the modulation function must be strictly positive, $h(\cdot) \ge \varepsilon > 0$, to ensure the re-weighted graph $G'=(V,E,W')$ remains connected. Setting weights to zero risks shattering the manifold.
\end{axiom}

\begin{axiom}[Bounded Distortion]
The modulation function $h$ must be bounded, i.e., $h(\cdot) \in [\varepsilon, C]$ for some constant $C$. Unbounded modulation can lead to an unstable spectrum and cause the operator to behave pathologically.
\end{axiom}

\begin{axiom}[Scale-Awareness]
The modulation function should not depend on the absolute values of a geometric signal (e.g., curvature $\kappa$) if that signal's distribution is unknown or highly skewed. Reliance on global statistics of a non-Gaussian signal makes the operator brittle and sensitive to outliers.
\end{axiom}

\section{A Taxonomy of Heuristic Failures}
We systematically tested four plausible anisotropic heuristics on the 20 Newsgroups dataset. Each experiment was run across 10 random seeds. The isotropic SGWT baseline achieved a mean Adjusted Rand Index (ARI) of $\mathbf{0.418 \pm 0.022}$. Each heuristic failed by violating one or more of the above axioms, leading to a statistically significant degradation in performance, summarized in Table \ref{tab:results}.

% The NEW, STATISTICALLY RIGOROUS TABLE
\begin{table}[h!]
    \centering
    \caption{Summary of Adjusted Rand Index (ARI) for Heuristic-Based Anisotropic Operators on the 20 Newsgroups Dataset. Results are reported as mean $\pm$ standard deviation over 10 random seeds.}
    \label{tab:results}
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Representation Builder} & \textbf{Mean ARI $\pm$ Std. Dev.} & \textbf{Violated Axiom(s)} \\ \midrule
        \textbf{Isotropic SGWT (Baseline)} & \textbf{0.418 $\pm$ 0.022} & \textit{N/A (Geometrically Blind)} \\ \midrule
        ACMW (Hard Gating) & 0.265 $\pm$ 0.020 & 1 (Connectivity) \\
        CPAL (Soft Gating) & 0.113 $\pm$ 0.015 & 3 (Scale-Awareness) \\
        Community-Modulated & 0.198 $\pm$ 0.018 & 3 (Scale-Awareness) \\
        Rank-Modulated & 0.172 $\pm$ 0.017 & \textit{None (Insufficient Impact)} \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Failure Mode 1: Graph Fragmentation}
\textbf{Heuristic:} Hard Curvature Gating (ACMW). Modulate weights using Forman-Ricci curvature $\kappa$ via $h(\kappa) = \max(0, 1 + \alpha \kappa)$. The intent is to set $w'_{ij} \to 0$ for negatively curved "bridges."

\noindent\textbf{Result:} ARI = $0.265 \pm 0.020$.

\noindent\textbf{Analysis:} This operator violates \textbf{Axiom 1 (Connectivity Preservation)}. By setting a large fraction of edge weights to zero, the operator shatters the graph into thousands of disconnected components. Diagnostic analysis revealed that this heuristic disconnects over 2,500 nodes from the main graph component. The global manifold structure is lost, and the resulting representation is based only on hyper-local, fragmented information.

\subsection{Failure Mode 2: Global Geometric Warping}
\textbf{Heuristic:} Soft Curvature Gating (CPAL). To fix fragmentation, we used a connectivity-preserving sigmoid: $h(\kappa) = \varepsilon + (1-\varepsilon)\sigma(\alpha(\kappa - \kappa_0))$, with $\kappa_0 = \text{median}(\kappa)$.

\noindent\textbf{Result:} ARI = $0.113 \pm 0.015$.

\noindent\textbf{Analysis:} This operator violates \textbf{Axiom 3 (Scale-Awareness)}. Our diagnostic analysis (Figure \ref{fig:diagnostic_plots}a) shows that the distribution of Forman-Ricci curvature is highly non-Gaussian and long-tailed. Applying a global, non-linear sigmoid function centered on the median catastrophically warped the entire graph geometry. It aggressively and incorrectly re-weighted the majority of "normal" edges, leading to a diffusion process profoundly misaligned with the true semantic structure.

\subsection{Failure Mode 3: Information Loss via Coarse Partitioning}
\textbf{Heuristic:} Community-Based Modulation. Use the Louvain algorithm to find communities and dampen inter-community edges by a factor $\varepsilon$.

\noindent\textbf{Result:} ARI = $0.198 \pm 0.018$.

\noindent\textbf{Analysis:} This operator also violates \textbf{Axiom 3 (Scale-Awareness)}. The Louvain algorithm provides a single, coarse-grained view of the graph's structure. On 20 Newsgroups (20 ground-truth classes), it found only 16 communities. By aggressively dampening all inter-community edges, we discarded fine-grained information about topic overlap and incorrectly reinforced the boundaries of merged, heterogeneous communities, making the final clustering task harder.

\subsection{Failure Mode 4: Insufficient Global Impact}
\textbf{Heuristic:} Rank-Based Modulation. Modulate only the top and bottom 10\% of edges based on their curvature \textit{rank}.

\noindent\textbf{Result:} ARI = $0.172 \pm 0.017$.

\noindent\textbf{Analysis:} While this operator satisfies all axioms, it fails for a more subtle reason: the intervention is too weak. Modifying only a small fraction of edges does not sufficiently perturb the global spectrum of the graph Laplacian, as formalized by matrix perturbation theory (Theorem \ref{thm:hoffman}). The resulting representation is visually chaotic (Figure \ref{fig:diagnostic_plots}b), confirming that the operator makes a change that is too local to solve a global clustering problem.

\begin{theorem}[Spectral Insensitivity to Sparse Perturbations \cite{hoffman1953variation}]
\label{thm:hoffman}
Let $L$ be the normalized Laplacian and $L' = L+P$ be the perturbed operator, where $P$ is a sparse perturbation matrix. The change in the spectrum is bounded by the Frobenius norm of the perturbation: $\sum_{i=1}^N (\lambda'_i - \lambda_i)^2 \le \|P\|_F^2$. For our rank-based modulation, $\|P\|_F^2$ is small, and the resulting change to the global eigenvectors is insufficient to improve separation.
\end{theorem}

% THE NEW DIAGNOSTIC FIGURE BLOCK
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../paper/figures/curvature_distribution.png}
        \caption{Curvature Distribution}
        \label{fig:curvature_dist}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../../paper/figures/failed_heuristic_visualization.png}
        \caption{UMAP of Rank-Modulated Representation}
        \label{fig:failed_viz}
    \end{subfigure}
    \caption{(a) The highly skewed, non-Gaussian distribution of Forman-Ricci curvature on the 20 Newsgroups manifold, making global heuristics brittle. (b) A 2D UMAP projection of the representation from the Rank-Modulated operator. The clusters are poorly separated, visually confirming the low ARI score.}
    \label{fig:diagnostic_plots}
\end{figure}

\section{Conclusion: The Necessity of a Learned Approach}
We have systematically demonstrated that four distinct classes of intuitive, hand-crafted anisotropic operators fail to improve upon a simple isotropic baseline. The failures are not random but correspond to violations of a clear set of axioms required for a robust operator. The relationship between local geometry and optimal global information diffusion is too complex, non-linear, and data-dependent to be captured by a fixed heuristic.

This body of negative results is not an endpoint but a signpost. It definitively proves that if we are to harness the rich geometric signal present in data manifolds, we cannot impose our own simplistic priors upon it. We must build systems with the capacity to \textit{learn} the optimal, connectivity-preserving, and data-driven diffusion process. This work closes the chapter on naive heuristics and provides the formal and empirical justification for the pivot to a fully learnable framework.

\begin{thebibliography}{9}
    \bibitem{hammond2011wavelets}
    D. K. Hammond, P. Vandergheynst, and R. Gribonval,
    \textit{Wavelets on graphs via spectral graph theory.}
    Applied and Computational Harmonic Analysis, 30(2), 129-150, 2011.

    \bibitem{hoffman1953variation}
    A. J. Hoffman and H. W. Wielandt,
    \textit{The variation of the spectrum of a normal matrix.}
    Duke Mathematical Journal, 20(1), 37-39, 1953.
\end{thebibliography}

\end{document}
