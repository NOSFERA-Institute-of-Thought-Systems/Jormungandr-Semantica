\chapter{Introduction: A Geometric Reckoning for Unsupervised NLP}
\label{chap:introduction}

The central project of modern computational linguistics is the automated discovery of meaning in vast, unstructured textual data. This pursuit, once the domain of symbolic logic and rule-based systems, has been revolutionized by a statistical paradigm, which posits that a word's meaning is encoded in its patterns of co-occurrence with other words. This principle gave rise to foundational models like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) \citep{blei2003latent}, which constructed "topic spaces" from document-term matrices. The deep learning era refined this paradigm, replacing sparse word counts with dense, continuous vector representations from models like Word2Vec \citep{pennington2014glove} and BERT \citep{devlin2018bert}. This shifted the locus of meaning from discrete co-occurrences to geometric proximity in a high-dimensional embedding space.

The contemporary state-of-the-art, exemplified by powerful neural topic models like BERTopic \citep{grootendorst2022bertopic}, represents the pinnacle of this "vector space semantics" philosophy. These models combine pre-trained sentence embeddings with sophisticated clustering algorithms to identify thematic groups. Their success is undeniable, yet they operate on a fundamental, and perhaps limiting, assumption: that the geometry of the embedding space is Euclidean and that the shortest path between two points is a straight line. They treat the data as a "cloud" of points, where metric distance is the final arbiter of semantic similarity.

This dissertation challenges that assumption. We argue that the geometry of meaning is not flat, but curved. We build upon the **Manifold Hypothesis** \citep{belkin2003laplacian}, which conjectures that real-world high-dimensional data, far from filling its ambient space, lies on or near a much lower-dimensional, non-linear manifold. A corpus of documents, therefore, is not a simple cloud but a complex geometric object—a "text manifold"—with intrinsic structures like thematic clusters appearing as dense regions, nuanced sub-topics as branching threads, and conceptual voids as sparse patches. Methods that rely solely on Euclidean distance may fail to respect this intrinsic structure, conflating points that are close in the ambient space but geodesically distant along the manifold (e.g., a document that bridges two distinct scientific fields).

This work proposes a fundamental shift in perspective: from the statistical analysis of point clouds to the **signal processing on data manifolds**. We introduce **Jörmungandr-Semantica**, a comprehensive theoretical and algorithmic framework built upon this geometric principle. Our approach treats documents as nodes in a graph, a discrete approximation of the text manifold. This graph is not merely a data structure; it is a scaffold upon which we can deploy the powerful analytical machinery of Graph Signal Processing (GSP) \citep{shuman2013emerging}.

The core technical innovation of this dissertation is the application of the **Spectral Graph Wavelet Transform (SGWT)** \citep{hammond2011wavelets} to signals defined on this text manifold. Unlike the Graph Fourier Transform, which provides a global decomposition into graph frequencies, wavelets offer a multi-scale analysis that is localized in both "space" (regions of the graph) and "scale" (thematic granularity). This allows our representations to simultaneously capture fine-grained distinctions between semantically similar documents and understand their role within the broader thematic organization of the entire corpus. This multi-scale geometric inductive bias, we contend, is the key to unlocking a more robust and interpretable model of semantic structure.

\subsection{A Unifying Central Theme}
The intellectual through-line of this dissertation is the development and validation of **geometric signal processing as a foundational paradigm for unsupervised representation learning and scientific discovery**. We aim to demonstrate that by explicitly modeling the geometry of data, we can design algorithms that are not only higher-performing on benchmark tasks but also serve as novel scientific instruments—"geometric telescopes"—for exploring the structure of complex systems.

\subsection{Pillars of Contribution}
This dissertation is built upon five interconnected, original contributions that span theory, algorithms, empirical validation, and application:

\begin{enumerate}
    \item \textbf{A Rigorous Theoretical Pillar:} We move beyond heuristic justification and establish a formal mathematical foundation for our framework. We introduce and prove two core theorems: a \textbf{Wavelet Stability Theorem} that guarantees robustness to data perturbations, and a \textbf{Geometric Clusterability Theorem} that forges a novel, verifiable link between the Ollivier-Ricci curvature of the text manifold and its spectral clusterability. This provides a geometric explanation for the intrinsic difficulty of clustering certain corpora.

    \item \textbf{Algorithmic Innovations:} We present not only the core Jörmungandr-Semantica pipeline but also novel algorithmic extensions. This includes the design of \textbf{adaptive wavelet kernels} that tailor the analysis to the local geometry of the graph and the development of a \textbf{scalable curvature estimation algorithm} that makes geometric analysis feasible for graphs with millions of nodes.

    \item \textbf{State-of-the-Art Benchmarking \& Engineering:} We conduct a large-scale, rigorous empirical study against modern baselines. We establish the statistically significant superiority of our method on standard benchmarks and, in the spirit of open science, release a high-performance, open-source Python library with a fully reproducible, one-click workflow via a public Docker image and Kaggle notebooks.

    \item \textbf{A Meta-Science Application:} We demonstrate the framework's utility as a tool for the "science of science." Our major case study is a longitudinal analysis of the arXiv preprint server, where we use changes in manifold curvature and geodesic distance to quantitatively map the evolution of scientific paradigms, such as the convergence of the NLP and Computer Vision fields.

    \item \textbf{Cross-Disciplinary Generalization:} We showcase the power of our geometric perspective by applying it to a radically different domain: single-cell genomics. We demonstrate that the same framework can be used to identify cell types and developmental trajectories from single-cell RNA sequencing data, highlighting the universality of the geometric approach.
\end{enumerate}

\subsection{Dissertation Outline}
This work is structured in four parts. \textbf{Part I} lays the foundations, reviewing the mathematical preliminaries of graph signal processing and presenting our core theoretical contributions. \textbf{Part II} details the Jörmungandr-Semantica framework, from the baseline pipeline to our novel algorithmic and scalability innovations. \textbf{Part III} is dedicated to empirical validation and applications, presenting the results of our benchmark experiments and our major case studies in scientometrics and genomics. Finally, \textbf{Part IV} provides a visionary outlook, discussing the limitations of our current work and positioning it within the long-term evolution of artificial intelligence and computational science.