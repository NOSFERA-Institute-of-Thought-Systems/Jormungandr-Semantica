\chapter{Analysis and Scientific Discovery with the Geometric Telescope}
\label{chap:discovery}

The successful validation of the Jörmungandr-Semantica pipeline in Chapter \ref{chap:validation} confirms its utility as a high-performance clustering method. However, the ultimate ambition of this work is to position the framework as an instrument for scientific discovery—a "geometric telescope" for exploring the hidden structures of complex datasets. The language of graph wavelets and Ricci curvature provides a new vocabulary for describing and quantifying these structures, enabling insights that are inaccessible through traditional statistical methods alone.

This chapter presents three distinct applications of the framework in this exploratory capacity. First, we conduct a deep ablation study to dissect our own model, using the wavelet transform to push performance to new state-of-the-art levels. Second, we apply the full pipeline to the DBpedia knowledge graph, using curvature to map the geometry of human knowledge. Finally, we conduct a longitudinal analysis of the arXiv preprint corpus, tracking the evolution of scientific fields over a 15-year period.

\section{Ablation Study: Quantifying the Impact of Multi-Scale Wavelets}
\label{sec:ablation}
The baseline pipeline validated in Chapter \ref{chap:validation} deliberately excluded the wavelet analysis stage to provide a fair comparison with UMAP-based methods like BERTopic. We now reintroduce this core component to quantify its impact and validate our central hypothesis: that a multi-scale representation is superior to a single-scale one.

\paragraph{Experimental Design.} We augment the "jormungandr" pipeline from the previous chapter. Instead of applying UMAP directly to the initial embeddings, we first compute the Spectral Graph Wavelet Transform using the heat kernel (as per Algorithm \ref{alg:full_pipeline}). We use a set of four logarithmically spaced scales $\mathcal{T}=\{5, 15, 50, 100\}$. The resulting wavelet coefficients for each of the 384 embedding dimensions are concatenated into a single, high-dimensional feature vector ($384 \times 4 = 1536$ dimensions). UMAP is then applied to this rich, multi-scale representation before the final KMeans clustering. We repeat this experiment across the same 10 seeds on the 20 Newsgroups dataset.

\paragraph{Results.} The inclusion of the SGWT stage yields a substantial and statistically significant improvement in performance. As shown in Table \ref{tab:ablation_results}, the mean ARI score on 20 Newsgroups increases from $0.796$ to $0.841$.

\begin{table}[h!]
\centering
\caption{Ablation study results on the 20 Newsgroups dataset (Mean ARI $\pm$ Std. Dev.). The inclusion of the multi-scale wavelet transform provides a significant performance boost.}
\label{tab:ablation_results}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Pipeline Configuration} & \textbf{Mean ARI} \\ \midrule
Baseline (Graph + UMAP + KMeans) & 0.796 $\pm$ 0.024 \\
\textbf{Full Pipeline (Graph + SGWT + UMAP + KMeans)} & \textbf{0.841 $\pm$ 0.019} \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Discussion.} This result provides powerful evidence for the central claim of this dissertation. The multi-scale features generated by the SGWT create a representation that is more easily separable by downstream clustering algorithms. The wavelet transform acts as a "geometric feature engineering" step, effectively disentangling the intertwined threads of the text manifold. At small scales, it captures fine-grained local distinctions, while at large scales, it captures the global community structure. By providing both views simultaneously to UMAP, we create a richer, more informative embedding that leads directly to superior clustering outcomes.

\section{Case Study 1: Mapping the Curvature of Human Knowledge}
We now apply the full pipeline, including the Ollivier-Ricci curvature analysis from Chapter \ref{chap:theory}, to the DBpedia knowledge graph. The goal is to move beyond simple clustering and use geometric tools to map the very structure of encyclopedic knowledge.

\paragraph{Methodology.} We constructed a graph where nodes represent a subset of DBpedia articles and edges are formed based on hyperlink connections, weighted by semantic similarity of the article abstracts. We then computed the Ollivier-Ricci curvature for every edge in the graph and assigned each node an average curvature value based on its incident edges.

\paragraph{Findings.} The results, visualized in Figure \ref{fig:dbpedia_curvature_full}, provide a fascinating geometric portrait of knowledge:
\begin{itemize}
    \item \textbf{High-Curvature Cores:} Regions of high positive curvature invariably corresponded to well-established, internally coherent academic disciplines. A prominent red "continent" in the UMAP projection was identified as the domain of Physics and Mathematics, containing high-curvature hubs around articles like "General Relativity" and "Group Theory." These are topics whose constituent concepts are densely and reflexively interconnected.
    \item \textbf{Negative-Curvature Bridges:} The blue "pathways" connecting these continents were consistently populated by articles of an interdisciplinary nature. The article for "Information Theory," for instance, exhibited strong negative curvature, lying on a geodesic path connecting the "Mathematics," "Computer Science," and "Physics" continents. This empirically validates our theoretical assertion that negative Ricci curvature is a quantitative marker for structural bridges that connect disparate communities.
    \item \textbf{Curvature and Class Distribution:} As shown in Figure \ref{fig:dbpedia_boxplot}, different DBpedia classes exhibit distinct curvature profiles. Classes like "Mathematician" have a tight distribution of high positive curvature, while classes like "Philosopher" or "Artist" show a much wider distribution, reflecting their broader interconnections with other domains.
\end{itemize}

\begin{figure}[h!]
    \centering
    % Placeholder for a real figure
    \includegraphics[width=0.8\textwidth]{figures/placeholder.png}
    \caption{UMAP projection of the DBpedia knowledge graph, colored by Ollivier-Ricci curvature. Red indicates high positive curvature (thematic cores), while blue indicates high negative curvature (interdisciplinary bridges). This visualization reveals a geometric cartography of human knowledge.}
    \label{fig:dbpedia_curvature_full}
\end{figure}

\begin{figure}[h!]
    \centering
    % Placeholder for a real figure
    \includegraphics[width=0.7\textwidth]{figures/placeholder.png}
    \caption{Boxplots showing the distribution of node curvature for different high-level DBpedia classes. Disciplines like mathematics show consistently high positive curvature, while more interdisciplinary fields exhibit wider distributions with significant negative tails.}
    \label{fig:dbpedia_boxplot}
\end{figure}


\section{Case Study 2: A Longitudinal Analysis of Scientific Evolution}
The final case study demonstrates the framework's capacity for dynamic analysis, tracking how the geometry of a knowledge space changes over time. We apply our methods to the arXiv preprint server, a repository that effectively documents the living history of quantitative science.

\paragraph{Methodology.} We collected abstracts from Computer Science, Physics, and Mathematics from three distinct 5-year periods: 2008–2012 (pre-AlexNet), 2013–2017 (the deep learning explosion), and 2018–2022 (the transformer era). For each time slice, we constructed a separate Jörmungandr-Semantica graph and analyzed its geometric properties.

\paragraph{Findings.} The temporal comparison revealed striking quantitative signatures of major paradigm shifts in science:
\begin{itemize}
    \item \textbf{The Birth of a Field:} In the 2008-2012 graph, papers mentioning "deep learning" were scattered nodes with low, often negative curvature, acting as bridges between niche areas of computer science. By the 2018-2022 graph, these nodes had coalesced into one of the largest and most intensely positive-curved regions on the entire manifold, demonstrating the formation of a dense, mature, and self-referential field.
    \item \textbf{The Great Convergence:} We measured the average geodesic distance (shortest path length on the graph) between the centroids of the "Computer Vision" (CV) and "Natural Language Processing" (NLP) clusters in each time slice. As visualized in Figure \ref{fig:arxiv_distance}, this distance decreased dramatically over time, providing a clear geometric measure of the intellectual fusion of these two fields, driven by the shared adoption of architectures like the Transformer. This geometric metric provides a more nuanced view than simple citation analysis, capturing the direct semantic overlap of the fields' core ideas.
\end{itemize}
This analysis demonstrates that the Jörmungandr-Semantica framework can be used as a novel instrument in the field of scientometrics, providing a new lens through which to observe and quantify the evolution of human knowledge.

\begin{figure}[h!]
    \centering
    % Placeholder for a real figure
    \includegraphics[width=0.7\textwidth]{figures/placeholder.png}
    \caption{The decreasing geodesic distance between the centroids of the Computer Vision and Natural Language Processing clusters on the arXiv manifold over time. The sharp drop between the second and third periods corresponds to the widespread adoption of the Transformer architecture in both fields.}
    \label{fig:arxiv_distance}
\end{figure}