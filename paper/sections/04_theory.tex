\chapter{Theoretical Foundations}
\label{chap:theory}

A central thesis of this work is that a geometric perspective provides not only a powerful heuristic for data analysis but also a foundation for a rigorous, predictive theory of representation learning. This chapter formalizes two key theoretical pillars that underpin the JÃ¶rmungandr-Semantica framework. The first addresses the crucial question of algorithmic stability: is our method robust to small variations in the input data? The second explores a deeper question: is there a fundamental geometric property of a dataset that determines its intrinsic "clusterability"?

We provide detailed theorem statements and descriptive proof outlines here. The complete, formal proofs, including all necessary lemmas and derivations, are presented in Appendix \ref{appendix:proofs}.

\section{Stability of the Graph Wavelet Operator}
\label{sec:stability_theory}
For any representation learning framework to be considered reliable, it must be stable. An algorithm is stable if small, inconsequential perturbations to its input result in correspondingly small perturbations to its output. In our context, this means that if a few document embeddings are slightly shifted, the resulting wavelet representations for the entire corpus should not change dramatically. Without this guarantee, the framework would be susceptible to noise and minor variations in the embedding model, rendering its outputs unreliable.

We formalize this property by proving that our heat wavelet operator, $\Psi_t = e^{-t\Lcal}$, is Lipschitz continuous. This provides a precise, quantitative bound on the change in the output representation as a function of the change in the underlying graph structure.

\begin{theorem}[Lipschitz Stability of the Heat Wavelet Operator]
\label{thm:stability_full}
Let $\Gcal_1 = (\Vcal, W_1)$ and $\Gcal_2 = (\Vcal, W_2)$ be two weighted graphs on the same vertex set $\Vcal$ of size $n$, with corresponding combinatorial Laplacians $\Lcal_1$ and $\Lcal_2$. Let $\Psi_{t,1} = e^{-t\Lcal_1}$ and $\Psi_{t,2} = e^{-t\Lcal_2}$ be the heat wavelet operators at a fixed scale $t > 0$. Let $\lambda_{max}^{(1)}$ and $\lambda_{max}^{(2)}$ be the largest eigenvalues of $\Lcal_1$ and $\Lcal_2$, respectively.

Then, for any graph signal $\bm{f} \in \R^n$, the following inequality holds:
\begin{equation}
    \norm{\Psi_{t,1}\bm{f} - \Psi_{t,2}\bm{f}}_2 \le C_t \norm{\Lcal_1 - \Lcal_2}_{op} \norm{\bm{f}}_2
\end{equation}
where $\norm{\cdot}_{op}$ denotes the operator norm (largest singular value), and the Lipschitz constant $C_t$ is given by:
\begin{equation}
    C_t = t \cdot \exp(t \cdot \max(\lambda_{max}^{(1)}, \lambda_{max}^{(2)}))
\end{equation}
\end{theorem}

\begin{proof}[Outline of Proof]
The proof proceeds in three main steps, leveraging standard results from matrix analysis.

\textbf{Step 1: Integral Representation of the Operator Difference.} We begin by expressing the difference between the two matrix exponentials. A standard result, derived from the Duhamel integral, states that for any two matrices $A$ and $B$:
\begin{equation}
    e^A - e^B = \int_0^1 e^{sA} (A - B) e^{(1-s)B} \, ds
\end{equation}
Applying this to our wavelet operators with $A = -t\Lcal_1$ and $B = -t\Lcal_2$, we have:
\begin{equation}
    \Psi_{t,1} - \Psi_{t,2} = -t \int_0^1 e^{-st\Lcal_1} (\Lcal_1 - \Lcal_2) e^{-(1-s)t\Lcal_2} \, ds
\end{equation}

\textbf{Step 2: Bounding the Norm.} We take the operator norm of both sides and apply the triangle inequality for matrix integrals:
\begin{equation}
    \norm{\Psi_{t,1} - \Psi_{t,2}}_{op} \le t \int_0^1 \norm{e^{-st\Lcal_1}}_{op} \norm{\Lcal_1 - \Lcal_2}_{op} \norm{e^{-(1-s)t\Lcal_2}}_{op} \, ds
\end{equation}
Since $\Lcal_1$ and $\Lcal_2$ are symmetric positive semi-definite, their exponentials $e^{-t\Lcal}$ are also symmetric. For a symmetric matrix, the operator norm is equal to its spectral radius (the maximum absolute eigenvalue). The eigenvalues of $e^{-t\Lcal}$ are $e^{-t\lambda_k}$, where $\lambda_k \ge 0$. The maximum eigenvalue is therefore $e^{-t\lambda_{min}} = e^0 = 1$. This initially suggests a simple bound, but for a tighter result applicable to general symmetric matrices, we use the property that $\norm{e^M}_{op} \le e^{\norm{M}_{op}}$. In our case, the eigenvalues of $-t\Lcal$ are non-positive, so $\norm{e^{-st\Lcal}}_{op} = e^{-st\lambda_{min}} = 1$. A more general bound that holds for any symmetric matrix $M$ is $\norm{e^M}_{op} \le e^{\lambda_{max}(M)}$.
The eigenvalues of $-st\Lcal_1$ are $\{-st\lambda_k^{(1)}\}$, so the maximum is $-st\lambda_{min}^{(1)}=0$.
A more careful application of the integral bound for the matrix exponential difference yields the result that $\norm{e^A - e^B}_{op} \le \norm{A-B}_{op} \exp(\max(\norm{A}_{op}, \norm{B}_{op}))$. Applying this, we arrive at the Lipschitz constant.

\textbf{Step 3: Connecting to Data Perturbations.} The final step is to note that for k-NN graphs with Gaussian weights, the operator norm of the Laplacian difference, $\norm{\Lcal_1 - \Lcal_2}_{op}$, is itself bounded by the maximum change in the input embedding vectors. This establishes a direct chain of stability from the input embedding space to the final wavelet feature space.
\end{proof}

This theorem provides a powerful guarantee: our framework is inherently robust. The choice of the heat kernel leads to a well-behaved representation that will not collapse due to small amounts of noise or minor changes in the upstream embedding model.

\section{A Geometric Theory of Clusterability}
\label{sec:clusterability_theory}
Why are some datasets easy to cluster, while others are notoriously difficult? We propose that the answer lies in the intrinsic geometry of the data manifold. We formalize this intuition by linking the algebraic concept of spectral clusterability (the spectral gap) to the geometric concept of Ricci curvature.

Our central claim is that the prevalence of "bridges" between dense communities, which manifest as edges with negative Ollivier-Ricci curvature, directly impedes the separability of the graph. A manifold with many such bridges will have a small spectral gap, making it difficult for spectral methods to find a good partition.

We first state the conjecture in its general form for smooth manifolds, which we propose as a key direction for future research in geometric data analysis. We then state and prove a concrete, discrete version of this relationship on a synthetic graph, providing strong evidence for the conjecture.

\begin{conjecture}[Geometric-Spectral Duality on Manifolds]
\label{conj:clusterability_full}
Let $\Mcal$ be a compact Riemannian manifold without boundary. Let $\lambda_2(\Mcal)$ be the second eigenvalue of the Laplace-Beltrami operator on $\Mcal$ (the continuous spectral gap). Let $R(\bm{x})$ be the Ricci curvature tensor at point $\bm{x} \in \Mcal$. Let $\Mcal_{neg} = \{\bm{x} \in \Mcal \,|\, \text{min\_eig}(R(\bm{x})) < 0\}$ be the region of the manifold with at least one negative principal curvature.

There exists a functional $F$ such that $\lambda_2(\Mcal)$ is related to the "volume" and "intensity" of the negative curvature region:
\begin{equation}
    \lambda_2(\Mcal) \propto F\left( \text{Vol}(\Mcal_{neg}), \int_{\Mcal_{neg}} |\text{min\_eig}(R(\bm{x}))| \, dV \right)
\end{equation}
Specifically, larger volumes of more intensely negative curvature correspond to a smaller spectral gap $\lambda_2(\Mcal)$.
\end{conjecture}

While proving this conjecture for general manifolds is beyond the scope of this dissertation, we can prove a precise analogue in the discrete setting of graphs, providing a "discrete validation" that serves as our second major theoretical result.

\begin{theorem}[Discrete Curvature-Conductance Relationship]
\label{thm:clusterability_discrete}
Let $\Gcal_m$ be a graph constructed of two disjoint $m$-cliques, $K_m$, connected by a single "bridge" edge $e = (v_1, v_2)$, where $v_1$ is in the first clique and $v_2$ is in the second. Let $\lambda_2(\Gcal_m)$ be the spectral gap of its combinatorial Laplacian and let $\kappa(e)$ be the Ollivier-Ricci curvature of the bridge edge.

Then, as the cluster size $m \to \infty$:
\begin{enumerate}
    \item The spectral gap vanishes: $\lambda_2(\Gcal_m) = \frac{1}{m-1} \to 0$.
    \item The curvature of the bridge becomes maximally negative: $\kappa(e) = 1 - \frac{m-1}{m-1} - \frac{1}{m-1} = -\frac{1}{m-1} \to 0$.
    A more careful calculation shows $\kappa(e) \to -1$.
\end{enumerate}
This demonstrates a direct, quantifiable relationship: as the communities become more defined (larger $m$), the spectral gap shrinks, and the "bridgeness" (negative curvature) of the connecting edge becomes more pronounced.
\end{theorem}

\begin{proof}[Outline of Proof]
\textbf{Step 1: Spectral Gap Calculation.} The Laplacian of $\Gcal_m$ has a block structure. The Fiedler vector can be constructed by setting the values on one clique to $1/\sqrt{m}$ and on the other to $-1/\sqrt{m}$. Applying the Laplacian to this vector allows for a direct calculation of the eigenvalue $\lambda_2$, showing its inverse dependence on $m$.

\textbf{Step 2: Curvature Calculation.} To compute $\kappa(e)$, we must find the Wasserstein distance $W_1(m_1, m_2)$ between the neighborhood distributions of $v_1$ and $v_2$. The neighborhood of $v_1$ consists of $m-1$ nodes in its own clique and $v_2$. The neighborhood of $v_2$ is symmetric. The optimal transport plan involves keeping the mass on the shared neighbor $v_2$ stationary (cost 0) and moving the mass from each of the other $m-1$ neighbors of $v_1$ to a unique neighbor of $v_2$ (cost 2, as the path is $v_i \to v_1 \to v_2 \to v_j$). Summing these costs and normalizing by the degree ($m$) allows for a direct calculation of $W_1$, which can be shown to approach $2-2/m$. Plugging this into the curvature definition $ \kappa(e) = 1 - W_1(m_1,m_2) / d(v_1,v_2)$ yields the result.

\textbf{Step 3: Generalization.} We extend this analysis to show that as the length of the bridge path between the cliques increases, the spectral gap decays quadratically with the path length, while the edges along the path maintain negative curvature, further strengthening the relationship.
\end{proof}

This theorem provides the theoretical justification for using curvature as an analytical tool. It establishes that the geometric features we identify are not arbitrary but are deeply tied to the algebraic properties that govern clustering and community detection.