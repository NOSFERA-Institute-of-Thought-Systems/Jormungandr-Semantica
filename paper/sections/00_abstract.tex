\begin{abstract}
The unsupervised extraction of thematic structure from large text corpora remains a fundamental challenge in natural language processing. Traditional methods, from Latent Dirichlet Allocation to modern neural topic models like BERTopic, often rely on statistical co-occurrences or proximity in a global embedding space, potentially overlooking the complex, non-linear geometric relationships within the data. In this work, we introduce JÃ¶rmungandr-Semantica, a comprehensive framework that re-frames unsupervised text analysis as a problem of geometric signal processing on data manifolds. We model a text corpus as a point cloud of sentence embeddings, from which we construct a k-Nearest Neighbor graph as a discrete approximation of the underlying manifold. The core of our contribution lies in leveraging the Spectral Graph Wavelet Transform (SGWT) to analyze signals on this graph, yielding multi-scale representations that simultaneously capture local neighborhood structure and global thematic organization. We provide a rigorous theoretical foundation for this approach, introducing two novel theorems that link the stability of our wavelet operator to perturbations in the data and connect the geometric clusterability of the graph to its Ollivier-Ricci curvature distribution. Empirically, we demonstrate that a baseline implementation of our framework achieves statistically significant improvements in clustering quality (p < 0.005) over strong modern baselines, including BERTopic and HDBSCAN, on standard benchmark datasets. Finally, we showcase the framework's utility as a "geometric telescope" for scientific discovery, revealing the evolution of research fields in the arXiv corpus through changes in manifold curvature. Our work establishes the viability and power of a geometric, multi-scale perspective for unsupervised representation learning on text.
\end{abstract}